{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bibliotecas importadas com sucesso!\n",
            "Pandas version: 2.3.2\n",
            "NumPy version: 2.3.3\n",
            "Scikit-learn version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Preparação de Dados para Modelo de Machine Learning - Diabetes\n",
        "# Objetivo: Tratar outliers, encoding de variáveis categóricas e normalização\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Configurações\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Scikit-learn version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CARREGAMENTO E ANÁLISE INICIAL DOS DADOS\n",
            "============================================================\n",
            "Shape do dataset: (100000, 31)\n",
            "Número de linhas: 100,000\n",
            "Número de colunas: 31\n",
            "\n",
            "Variáveis numéricas (24): ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'glucose_fasting', 'glucose_postprandial', 'insulin_level', 'hba1c', 'diabetes_risk_score', 'diagnosed_diabetes']\n",
            "Variáveis categóricas (7): ['gender', 'ethnicity', 'education_level', 'income_level', 'employment_status', 'smoking_status', 'diabetes_stage']\n",
            "\n",
            "Variável target: diagnosed_diabetes\n",
            "Distribuição da classe target:\n",
            "diagnosed_diabetes\n",
            "1    59998\n",
            "0    40002\n",
            "Name: count, dtype: int64\n",
            "Proporção: diagnosed_diabetes\n",
            "1    0.6\n",
            "0    0.4\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Valores nulos por coluna:\n",
            "✅ Nenhum valor nulo encontrado!\n"
          ]
        }
      ],
      "source": [
        "# Carregamento dos dados\n",
        "print(\"=\"*60)\n",
        "print(\"CARREGAMENTO E ANÁLISE INICIAL DOS DADOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Carregar dados\n",
        "df = pd.read_csv('../data/raw/diabetes_dataset.csv')\n",
        "\n",
        "print(f\"Shape do dataset: {df.shape}\")\n",
        "print(f\"Número de linhas: {df.shape[0]:,}\")\n",
        "print(f\"Número de colunas: {df.shape[1]}\")\n",
        "\n",
        "# Identificar tipos de variáveis\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nVariáveis numéricas ({len(numeric_columns)}): {numeric_columns}\")\n",
        "print(f\"Variáveis categóricas ({len(categorical_columns)}): {categorical_columns}\")\n",
        "\n",
        "# Verificar variável target\n",
        "target_column = 'diagnosed_diabetes'\n",
        "if target_column in df.columns:\n",
        "    print(f\"\\nVariável target: {target_column}\")\n",
        "    print(\"Distribuição da classe target:\")\n",
        "    print(df[target_column].value_counts())\n",
        "    print(f\"Proporção: {df[target_column].value_counts(normalize=True).round(3)}\")\n",
        "else:\n",
        "    print(\"⚠️ Variável target 'diagnosed_diabetes' não encontrada!\")\n",
        "\n",
        "# Verificar valores nulos\n",
        "print(f\"\\nValores nulos por coluna:\")\n",
        "null_counts = df.isnull().sum()\n",
        "if null_counts.sum() > 0:\n",
        "    print(null_counts[null_counts > 0])\n",
        "else:\n",
        "    print(\"✅ Nenhum valor nulo encontrado!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DIVISÃO ESTRATIFICADA DOS DADOS\n",
            "============================================================\n",
            "Features (X): (100000, 30)\n",
            "Target (y): (100000,)\n",
            "\n",
            "Divisão dos dados:\n",
            "X_train: (80000, 30) (80.0%)\n",
            "X_test: (20000, 30) (20.0%)\n",
            "y_train: (80000,)\n",
            "y_test: (20000,)\n",
            "\n",
            "Distribuição da classe target:\n",
            "Train set:\n",
            "diagnosed_diabetes\n",
            "1    0.6\n",
            "0    0.4\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test set:\n",
            "diagnosed_diabetes\n",
            "1    0.6\n",
            "0    0.4\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "✅ Divisão estratificada concluída com sucesso!\n",
            "Random state: 42 (para reprodutibilidade)\n"
          ]
        }
      ],
      "source": [
        "# Divisão estratificada dos dados (Train/Test)\n",
        "print(\"=\"*60)\n",
        "print(\"DIVISÃO ESTRATIFICADA DOS DADOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separar features e target\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column]\n",
        "\n",
        "print(f\"Features (X): {X.shape}\")\n",
        "print(f\"Target (y): {y.shape}\")\n",
        "\n",
        "# Divisão estratificada 80/20\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nDivisão dos dados:\")\n",
        "print(f\"X_train: {X_train.shape} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"X_test: {X_test.shape} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "\n",
        "# Verificar se a estratificação funcionou\n",
        "print(f\"\\nDistribuição da classe target:\")\n",
        "print(\"Train set:\")\n",
        "print(y_train.value_counts(normalize=True).round(3))\n",
        "print(\"\\nTest set:\")\n",
        "print(y_test.value_counts(normalize=True).round(3))\n",
        "\n",
        "# Salvar índices para referência\n",
        "train_indices = X_train.index\n",
        "test_indices = X_test.index\n",
        "\n",
        "print(f\"\\n✅ Divisão estratificada concluída com sucesso!\")\n",
        "print(f\"Random state: 42 (para reprodutibilidade)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ANÁLISE DETALHADA DE OUTLIERS\n",
            "============================================================\n",
            "Análise de outliers no conjunto de TREINO:\n",
            "--------------------------------------------------\n",
            "RESUMO DE OUTLIERS POR VARIÁVEL:\n",
            "                              Variavel  Outliers_IQR  Pct_IQR  \\\n",
            "6              family_history_diabetes         17555    21.94   \n",
            "8               cardiovascular_history          6341     7.93   \n",
            "2   physical_activity_minutes_per_week          2607     3.26   \n",
            "22                 diabetes_risk_score           739     0.92   \n",
            "4                  sleep_hours_per_day           728     0.91   \n",
            "13                          heart_rate           704     0.88   \n",
            "9                                  bmi           598     0.75   \n",
            "18                     glucose_fasting           590     0.74   \n",
            "12                        diastolic_bp           583     0.73   \n",
            "21                               hba1c           494     0.62   \n",
            "19                glucose_postprandial           488     0.61   \n",
            "15                     hdl_cholesterol           462     0.58   \n",
            "11                         systolic_bp           418     0.52   \n",
            "1         alcohol_consumption_per_week           372     0.46   \n",
            "3                           diet_score           276     0.34   \n",
            "16                     ldl_cholesterol           276     0.34   \n",
            "14                   cholesterol_total           252     0.32   \n",
            "20                       insulin_level           239     0.30   \n",
            "5            screen_time_hours_per_day           237     0.30   \n",
            "17                       triglycerides           228     0.29   \n",
            "10                  waist_to_hip_ratio           212     0.26   \n",
            "0                                  age             0     0.00   \n",
            "7                 hypertension_history             0     0.00   \n",
            "\n",
            "    Outliers_ZScore  Pct_ZScore  \n",
            "6                 0        0.00  \n",
            "8              6341        7.93  \n",
            "2              1099        1.37  \n",
            "22              107        0.13  \n",
            "4               106        0.13  \n",
            "13              232        0.29  \n",
            "9                99        0.12  \n",
            "18              246        0.31  \n",
            "12              222        0.28  \n",
            "21              185        0.23  \n",
            "19              118        0.15  \n",
            "15              255        0.32  \n",
            "11              153        0.19  \n",
            "1               372        0.46  \n",
            "3               116        0.14  \n",
            "16              177        0.22  \n",
            "14              129        0.16  \n",
            "20              188        0.24  \n",
            "5               115        0.14  \n",
            "17              126        0.16  \n",
            "10              212        0.26  \n",
            "0                 0        0.00  \n",
            "7                 0        0.00  \n",
            "\n",
            "Variáveis com mais de 5% de outliers: ['family_history_diabetes', 'cardiovascular_history']\n",
            "\n",
            "Estatísticas detalhadas das variáveis com mais outliers:\n",
            "\n",
            "family_history_diabetes:\n",
            "  Outliers IQR: 17555 (21.94%)\n",
            "  Limites: [0.00, 0.00]\n",
            "  Min/Max no dataset: [0.00, 1.00]\n",
            "\n",
            "cardiovascular_history:\n",
            "  Outliers IQR: 6341 (7.93%)\n",
            "  Limites: [0.00, 0.00]\n",
            "  Min/Max no dataset: [0.00, 1.00]\n"
          ]
        }
      ],
      "source": [
        "# Análise detalhada de outliers\n",
        "print(\"=\"*60)\n",
        "print(\"ANÁLISE DETALHADA DE OUTLIERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detecta outliers usando o método IQR\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
        "\n",
        "def detect_outliers_zscore(data, column, threshold=3):\n",
        "    \"\"\"Detecta outliers usando Z-score\"\"\"\n",
        "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
        "    outliers = data[z_scores > threshold]\n",
        "    return outliers\n",
        "\n",
        "# Análise de outliers no conjunto de treino\n",
        "print(\"Análise de outliers no conjunto de TREINO:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "outlier_analysis = []\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in numeric_columns:\n",
        "    if col != target_column:  # Excluir variável target\n",
        "        # Método IQR\n",
        "        outliers_iqr, lower, upper, Q1, Q3, IQR = detect_outliers_iqr(X_train, col)\n",
        "        n_outliers_iqr = len(outliers_iqr)\n",
        "        pct_outliers_iqr = (n_outliers_iqr / len(X_train)) * 100\n",
        "        \n",
        "        # Método Z-score\n",
        "        outliers_zscore = detect_outliers_zscore(X_train, col)\n",
        "        n_outliers_zscore = len(outliers_zscore)\n",
        "        pct_outliers_zscore = (n_outliers_zscore / len(X_train)) * 100\n",
        "        \n",
        "        outlier_analysis.append({\n",
        "            'Variavel': col,\n",
        "            'Outliers_IQR': n_outliers_iqr,\n",
        "            'Pct_IQR': pct_outliers_iqr,\n",
        "            'Outliers_ZScore': n_outliers_zscore,\n",
        "            'Pct_ZScore': pct_outliers_zscore,\n",
        "            'Q1': Q1,\n",
        "            'Q3': Q3,\n",
        "            'IQR': IQR,\n",
        "            'Limite_Inferior': lower,\n",
        "            'Limite_Superior': upper\n",
        "        })\n",
        "        \n",
        "        outlier_summary[col] = {\n",
        "            'iqr_outliers': n_outliers_iqr,\n",
        "            'iqr_pct': pct_outliers_iqr,\n",
        "            'zscore_outliers': n_outliers_zscore,\n",
        "            'zscore_pct': pct_outliers_zscore,\n",
        "            'bounds': (lower, upper)\n",
        "        }\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_analysis)\n",
        "outlier_df = outlier_df.sort_values('Pct_IQR', ascending=False)\n",
        "\n",
        "print(\"RESUMO DE OUTLIERS POR VARIÁVEL:\")\n",
        "print(outlier_df[['Variavel', 'Outliers_IQR', 'Pct_IQR', 'Outliers_ZScore', 'Pct_ZScore']].round(2))\n",
        "\n",
        "# Identificar variáveis com muitos outliers\n",
        "high_outlier_vars = outlier_df[outlier_df['Pct_IQR'] > 5]['Variavel'].tolist()\n",
        "print(f\"\\nVariáveis com mais de 5% de outliers: {high_outlier_vars}\")\n",
        "\n",
        "# Mostrar estatísticas das variáveis com mais outliers\n",
        "if high_outlier_vars:\n",
        "    print(f\"\\nEstatísticas detalhadas das variáveis com mais outliers:\")\n",
        "    for var in high_outlier_vars[:5]:  # Mostrar apenas as 5 primeiras\n",
        "        stats_info = outlier_summary[var]\n",
        "        print(f\"\\n{var}:\")\n",
        "        print(f\"  Outliers IQR: {stats_info['iqr_outliers']} ({stats_info['iqr_pct']:.2f}%)\")\n",
        "        print(f\"  Limites: [{stats_info['bounds'][0]:.2f}, {stats_info['bounds'][1]:.2f}]\")\n",
        "        print(f\"  Min/Max no dataset: [{X_train[var].min():.2f}, {X_train[var].max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ESTRATÉGIA DE TRATAMENTO DE OUTLIERS\n",
            "============================================================\n",
            "Decisões de tratamento de outliers:\n",
            "----------------------------------------\n",
            "Variáveis que serão tratadas: ['family_history_diabetes', 'cardiovascular_history']\n",
            "\n",
            "Aplicando tratamento de outliers...\n",
            "  family_history_diabetes: [0.00, 0.00]\n",
            "  cardiovascular_history: [0.00, 0.00]\n",
            "\n",
            "✅ Tratamento de outliers concluído para 2 variáveis\n",
            "\n",
            "IMPACTO DO TRATAMENTO:\n",
            "------------------------------\n",
            "family_history_diabetes:\n",
            "  Original: [0.00, 1.00]\n",
            "  Tratado:  [0.00, 0.00]\n",
            "  Limites:  [0.00, 0.00]\n",
            "cardiovascular_history:\n",
            "  Original: [0.00, 1.00]\n",
            "  Tratado:  [0.00, 0.00]\n",
            "  Limites:  [0.00, 0.00]\n"
          ]
        }
      ],
      "source": [
        "# Estratégia de tratamento de outliers\n",
        "print(\"=\"*60)\n",
        "print(\"ESTRATÉGIA DE TRATAMENTO DE OUTLIERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def treat_outliers_capping(data, column, method='iqr', factor=1.5):\n",
        "    \"\"\"\n",
        "    Trata outliers usando capping (winsorization)\n",
        "    \n",
        "    Args:\n",
        "        data: DataFrame\n",
        "        column: nome da coluna\n",
        "        method: 'iqr' ou 'zscore'\n",
        "        factor: fator multiplicativo (1.5 para IQR, 3 para Z-score)\n",
        "    \"\"\"\n",
        "    data_copy = data.copy()\n",
        "    \n",
        "    if method == 'iqr':\n",
        "        Q1 = data_copy[column].quantile(0.25)\n",
        "        Q3 = data_copy[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - factor * IQR\n",
        "        upper_bound = Q3 + factor * IQR\n",
        "        \n",
        "    elif method == 'zscore':\n",
        "        mean = data_copy[column].mean()\n",
        "        std = data_copy[column].std()\n",
        "        lower_bound = mean - factor * std\n",
        "        upper_bound = mean + factor * std\n",
        "    \n",
        "    # Aplicar capping\n",
        "    data_copy[column] = np.where(data_copy[column] < lower_bound, lower_bound, data_copy[column])\n",
        "    data_copy[column] = np.where(data_copy[column] > upper_bound, upper_bound, data_copy[column])\n",
        "    \n",
        "    return data_copy, lower_bound, upper_bound\n",
        "\n",
        "# Decidir quais variáveis tratar\n",
        "print(\"Decisões de tratamento de outliers:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Variáveis que serão tratadas (com mais de 5% de outliers)\n",
        "vars_to_treat = high_outlier_vars.copy()\n",
        "\n",
        "# Adicionar variáveis importantes que podem ter outliers significativos\n",
        "important_vars = ['age', 'bmi', 'glucose_fasting', 'hba1c', 'diabetes_risk_score']\n",
        "for var in important_vars:\n",
        "    if var in numeric_columns and var not in vars_to_treat:\n",
        "        if outlier_summary[var]['iqr_pct'] > 2:  # Mais de 2% de outliers\n",
        "            vars_to_treat.append(var)\n",
        "\n",
        "vars_to_treat = list(set(vars_to_treat))  # Remover duplicatas\n",
        "print(f\"Variáveis que serão tratadas: {vars_to_treat}\")\n",
        "\n",
        "# Aplicar tratamento de outliers no conjunto de treino\n",
        "X_train_clean = X_train.copy()\n",
        "outlier_treatment_info = {}\n",
        "\n",
        "print(f\"\\nAplicando tratamento de outliers...\")\n",
        "for var in vars_to_treat:\n",
        "    if var in X_train_clean.columns:\n",
        "        X_train_clean, lower_bound, upper_bound = treat_outliers_capping(\n",
        "            X_train_clean, var, method='iqr', factor=1.5\n",
        "        )\n",
        "        \n",
        "        outlier_treatment_info[var] = {\n",
        "            'lower_bound': lower_bound,\n",
        "            'upper_bound': upper_bound,\n",
        "            'original_min': X_train[var].min(),\n",
        "            'original_max': X_train[var].max(),\n",
        "            'new_min': X_train_clean[var].min(),\n",
        "            'new_max': X_train_clean[var].max()\n",
        "        }\n",
        "        \n",
        "        print(f\"  {var}: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "\n",
        "print(f\"\\n✅ Tratamento de outliers concluído para {len(vars_to_treat)} variáveis\")\n",
        "\n",
        "# Mostrar impacto do tratamento\n",
        "print(f\"\\nIMPACTO DO TRATAMENTO:\")\n",
        "print(\"-\" * 30)\n",
        "for var in vars_to_treat[:5]:  # Mostrar apenas as 5 primeiras\n",
        "    info = outlier_treatment_info[var]\n",
        "    print(f\"{var}:\")\n",
        "    print(f\"  Original: [{info['original_min']:.2f}, {info['original_max']:.2f}]\")\n",
        "    print(f\"  Tratado:  [{info['new_min']:.2f}, {info['new_max']:.2f}]\")\n",
        "    print(f\"  Limites:  [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ENCODING DE VARIÁVEIS CATEGÓRICAS\n",
            "============================================================\n",
            "Análise das variáveis categóricas:\n",
            "----------------------------------------\n",
            "gender:\n",
            "  Valores únicos: 3\n",
            "  Mais comum: Female (50.2%)\n",
            "  Menos comum: Other (2.1%)\n",
            "  Distribuição: {'Female': np.int64(40182), 'Male': np.int64(38155), 'Other': np.int64(1663)}\n",
            "\n",
            "ethnicity:\n",
            "  Valores únicos: 5\n",
            "  Mais comum: White (44.9%)\n",
            "  Menos comum: Other (5.1%)\n",
            "  Distribuição: {'White': np.int64(35918), 'Hispanic': np.int64(15993), 'Black': np.int64(14467)}\n",
            "\n",
            "education_level:\n",
            "  Valores únicos: 4\n",
            "  Mais comum: Highschool (44.9%)\n",
            "  Menos comum: No formal (5.1%)\n",
            "  Distribuição: {'Highschool': np.int64(35935), 'Graduate': np.int64(28017), 'Postgraduate': np.int64(11972)}\n",
            "\n",
            "income_level:\n",
            "  Valores únicos: 5\n",
            "  Mais comum: Middle (35.2%)\n",
            "  Menos comum: High (5.0%)\n",
            "  Distribuição: {'Middle': np.int64(28122), 'Lower-Middle': np.int64(20126), 'Upper-Middle': np.int64(15881)}\n",
            "\n",
            "employment_status:\n",
            "  Valores únicos: 4\n",
            "  Mais comum: Employed (60.2%)\n",
            "  Menos comum: Student (6.2%)\n",
            "  Distribuição: {'Employed': np.int64(48120), 'Retired': np.int64(17395), 'Unemployed': np.int64(9549)}\n",
            "\n",
            "smoking_status:\n",
            "  Valores únicos: 3\n",
            "  Mais comum: Never (59.9%)\n",
            "  Menos comum: Former (20.0%)\n",
            "  Distribuição: {'Never': np.int64(47938), 'Current': np.int64(16094), 'Former': np.int64(15968)}\n",
            "\n",
            "diabetes_stage:\n",
            "  Valores únicos: 5\n",
            "  Mais comum: Type 2 (59.8%)\n",
            "  Menos comum: Type 1 (0.1%)\n",
            "  Distribuição: {'Type 2': np.int64(47815), 'Pre-Diabetes': np.int64(25492), 'No Diabetes': np.int64(6370)}\n",
            "\n",
            "ESTRATÉGIA DE ENCODING:\n",
            "------------------------------\n",
            "gender: One-Hot Encoding (3 categorias)\n",
            "ethnicity: One-Hot Encoding (5 categorias)\n",
            "education_level: One-Hot Encoding (4 categorias)\n",
            "income_level: One-Hot Encoding (5 categorias)\n",
            "employment_status: One-Hot Encoding (4 categorias)\n",
            "smoking_status: One-Hot Encoding (3 categorias)\n",
            "diabetes_stage: One-Hot Encoding (5 categorias)\n",
            "\n",
            "Resumo:\n",
            "  One-Hot Encoding: 7 variáveis\n",
            "  Label Encoding: 0 variáveis\n"
          ]
        }
      ],
      "source": [
        "# Encoding de variáveis categóricas\n",
        "print(\"=\"*60)\n",
        "print(\"ENCODING DE VARIÁVEIS CATEGÓRICAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Analisar variáveis categóricas\n",
        "print(\"Análise das variáveis categóricas:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "categorical_analysis = {}\n",
        "for col in categorical_columns:\n",
        "    unique_values = X_train[col].nunique()\n",
        "    value_counts = X_train[col].value_counts()\n",
        "    most_common = value_counts.iloc[0]\n",
        "    least_common = value_counts.iloc[-1]\n",
        "    \n",
        "    categorical_analysis[col] = {\n",
        "        'unique_count': unique_values,\n",
        "        'most_common': most_common,\n",
        "        'least_common': least_common,\n",
        "        'most_common_pct': (most_common / len(X_train)) * 100,\n",
        "        'least_common_pct': (least_common / len(X_train)) * 100\n",
        "    }\n",
        "    \n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  Valores únicos: {unique_values}\")\n",
        "    print(f\"  Mais comum: {value_counts.index[0]} ({categorical_analysis[col]['most_common_pct']:.1f}%)\")\n",
        "    print(f\"  Menos comum: {value_counts.index[-1]} ({categorical_analysis[col]['least_common_pct']:.1f}%)\")\n",
        "    print(f\"  Distribuição: {dict(value_counts.head(3))}\")\n",
        "    print()\n",
        "\n",
        "# Decidir estratégia de encoding para cada variável\n",
        "print(\"ESTRATÉGIA DE ENCODING:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "encoding_strategy = {}\n",
        "onehot_vars = []\n",
        "label_vars = []\n",
        "\n",
        "for col in categorical_columns:\n",
        "    unique_count = categorical_analysis[col]['unique_count']\n",
        "    \n",
        "    if unique_count <= 5:  # Poucas categorias - One-Hot Encoding\n",
        "        encoding_strategy[col] = 'onehot'\n",
        "        onehot_vars.append(col)\n",
        "        print(f\"{col}: One-Hot Encoding ({unique_count} categorias)\")\n",
        "    else:  # Muitas categorias - Label Encoding\n",
        "        encoding_strategy[col] = 'label'\n",
        "        label_vars.append(col)\n",
        "        print(f\"{col}: Label Encoding ({unique_count} categorias)\")\n",
        "\n",
        "print(f\"\\nResumo:\")\n",
        "print(f\"  One-Hot Encoding: {len(onehot_vars)} variáveis\")\n",
        "print(f\"  Label Encoding: {len(label_vars)} variáveis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "IMPLEMENTAÇÃO DO ENCODING\n",
            "============================================================\n",
            "Aplicando Label Encoding...\n",
            "\n",
            "Aplicando One-Hot Encoding...\n",
            "  gender: 2 colunas dummy criadas\n",
            "  ethnicity: 4 colunas dummy criadas\n",
            "  education_level: 3 colunas dummy criadas\n",
            "  income_level: 4 colunas dummy criadas\n",
            "  employment_status: 3 colunas dummy criadas\n",
            "  smoking_status: 2 colunas dummy criadas\n",
            "  diabetes_stage: 4 colunas dummy criadas\n",
            "\n",
            "✅ Encoding concluído!\n",
            "Shape original: (80000, 30)\n",
            "Shape após encoding: (80000, 45)\n",
            "Novas colunas criadas: 15\n",
            "\n",
            "Novas colunas: ['gender_Male', 'gender_Other', 'ethnicity_Black', 'ethnicity_Hispanic', 'ethnicity_Other', 'ethnicity_White', 'education_level_Highschool', 'education_level_No formal', 'education_level_Postgraduate', 'income_level_Low', 'income_level_Lower-Middle', 'income_level_Middle', 'income_level_Upper-Middle', 'employment_status_Retired', 'employment_status_Student', 'employment_status_Unemployed', 'smoking_status_Former', 'smoking_status_Never', 'diabetes_stage_No Diabetes', 'diabetes_stage_Pre-Diabetes', 'diabetes_stage_Type 1', 'diabetes_stage_Type 2']\n",
            "\n",
            "Valores nulos após encoding: 0\n",
            "✅ Nenhum valor nulo após encoding!\n"
          ]
        }
      ],
      "source": [
        "# Implementação do encoding\n",
        "print(\"=\"*60)\n",
        "print(\"IMPLEMENTAÇÃO DO ENCODING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Aplicar encoding no conjunto de treino\n",
        "X_train_encoded = X_train_clean.copy()\n",
        "\n",
        "# 1. Label Encoding para variáveis com muitas categorias\n",
        "print(\"Aplicando Label Encoding...\")\n",
        "label_encoders = {}\n",
        "\n",
        "for col in label_vars:\n",
        "    le = LabelEncoder()\n",
        "    X_train_encoded[col] = le.fit_transform(X_train_encoded[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"  {col}: {len(le.classes_)} categorias -> {le.classes_}\")\n",
        "\n",
        "# 2. One-Hot Encoding para variáveis com poucas categorias\n",
        "print(f\"\\nAplicando One-Hot Encoding...\")\n",
        "onehot_encoders = {}\n",
        "\n",
        "for col in onehot_vars:\n",
        "    # Criar dummies\n",
        "    dummies = pd.get_dummies(X_train_encoded[col], prefix=col, drop_first=True)\n",
        "    \n",
        "    # Remover coluna original e adicionar dummies\n",
        "    X_train_encoded = X_train_encoded.drop(columns=[col])\n",
        "    X_train_encoded = pd.concat([X_train_encoded, dummies], axis=1)\n",
        "    \n",
        "    onehot_encoders[col] = {\n",
        "        'categories': X_train_clean[col].unique(),\n",
        "        'dummy_columns': dummies.columns.tolist()\n",
        "    }\n",
        "    \n",
        "    print(f\"  {col}: {len(dummies.columns)} colunas dummy criadas\")\n",
        "\n",
        "print(f\"\\n✅ Encoding concluído!\")\n",
        "print(f\"Shape original: {X_train_clean.shape}\")\n",
        "print(f\"Shape após encoding: {X_train_encoded.shape}\")\n",
        "print(f\"Novas colunas criadas: {X_train_encoded.shape[1] - X_train_clean.shape[1]}\")\n",
        "\n",
        "# Mostrar novas colunas criadas\n",
        "new_columns = [col for col in X_train_encoded.columns if col not in X_train_clean.columns]\n",
        "print(f\"\\nNovas colunas: {new_columns}\")\n",
        "\n",
        "# Verificar se há valores nulos após encoding\n",
        "null_after_encoding = X_train_encoded.isnull().sum().sum()\n",
        "print(f\"\\nValores nulos após encoding: {null_after_encoding}\")\n",
        "if null_after_encoding > 0:\n",
        "    print(\"⚠️ Valores nulos encontrados após encoding!\")\n",
        "    print(X_train_encoded.isnull().sum()[X_train_encoded.isnull().sum() > 0])\n",
        "else:\n",
        "    print(\"✅ Nenhum valor nulo após encoding!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "NORMALIZAÇÃO/PADRONIZAÇÃO DE VARIÁVEIS NUMÉRICAS\n",
            "============================================================\n",
            "Variáveis numéricas após encoding: 23\n",
            "Colunas: ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'glucose_fasting', 'glucose_postprandial', 'insulin_level', 'hba1c', 'diabetes_risk_score']\n",
            "\n",
            "Análise das distribuições:\n",
            "----------------------------------------\n",
            "age:\n",
            "  Média: 50.10, Desvio: 15.59\n",
            "  Min: 18.00, Max: 90.00, Range: 72.00\n",
            "  CV: 0.311\n",
            "alcohol_consumption_per_week:\n",
            "  Média: 2.00, Desvio: 1.42\n",
            "  Min: 0.00, Max: 10.00, Range: 10.00\n",
            "  CV: 0.708\n",
            "physical_activity_minutes_per_week:\n",
            "  Média: 119.09, Desvio: 84.76\n",
            "  Min: 0.00, Max: 833.00, Range: 833.00\n",
            "  CV: 0.712\n",
            "diet_score:\n",
            "  Média: 5.99, Desvio: 1.78\n",
            "  Min: 0.00, Max: 10.00, Range: 10.00\n",
            "  CV: 0.297\n",
            "sleep_hours_per_day:\n",
            "  Média: 7.00, Desvio: 1.09\n",
            "  Min: 3.00, Max: 10.00, Range: 7.00\n",
            "  CV: 0.156\n",
            "screen_time_hours_per_day:\n",
            "  Média: 5.99, Desvio: 2.47\n",
            "  Min: 0.50, Max: 16.80, Range: 16.30\n",
            "  CV: 0.412\n",
            "family_history_diabetes:\n",
            "  Média: 0.00, Desvio: 0.00\n",
            "  Min: 0.00, Max: 0.00, Range: 0.00\n",
            "  CV: 0.000\n",
            "hypertension_history:\n",
            "  Média: 0.25, Desvio: 0.43\n",
            "  Min: 0.00, Max: 1.00, Range: 1.00\n",
            "  CV: 1.723\n",
            "cardiovascular_history:\n",
            "  Média: 0.00, Desvio: 0.00\n",
            "  Min: 0.00, Max: 0.00, Range: 0.00\n",
            "  CV: 0.000\n",
            "bmi:\n",
            "  Média: 25.61, Desvio: 3.59\n",
            "  Min: 15.00, Max: 39.20, Range: 24.20\n",
            "  CV: 0.140\n",
            "waist_to_hip_ratio:\n",
            "  Média: 0.86, Desvio: 0.05\n",
            "  Min: 0.67, Max: 1.06, Range: 0.39\n",
            "  CV: 0.055\n",
            "systolic_bp:\n",
            "  Média: 115.79, Desvio: 14.27\n",
            "  Min: 90.00, Max: 179.00, Range: 89.00\n",
            "  CV: 0.123\n",
            "diastolic_bp:\n",
            "  Média: 75.23, Desvio: 8.20\n",
            "  Min: 50.00, Max: 110.00, Range: 60.00\n",
            "  CV: 0.109\n",
            "heart_rate:\n",
            "  Média: 69.62, Desvio: 8.40\n",
            "  Min: 40.00, Max: 105.00, Range: 65.00\n",
            "  CV: 0.121\n",
            "cholesterol_total:\n",
            "  Média: 185.97, Desvio: 32.01\n",
            "  Min: 100.00, Max: 310.00, Range: 210.00\n",
            "  CV: 0.172\n",
            "hdl_cholesterol:\n",
            "  Média: 54.05, Desvio: 10.26\n",
            "  Min: 20.00, Max: 98.00, Range: 78.00\n",
            "  CV: 0.190\n",
            "ldl_cholesterol:\n",
            "  Média: 102.97, Desvio: 33.38\n",
            "  Min: 50.00, Max: 263.00, Range: 213.00\n",
            "  CV: 0.324\n",
            "triglycerides:\n",
            "  Média: 121.42, Desvio: 43.41\n",
            "  Min: 30.00, Max: 344.00, Range: 314.00\n",
            "  CV: 0.357\n",
            "glucose_fasting:\n",
            "  Média: 111.11, Desvio: 13.59\n",
            "  Min: 60.00, Max: 167.00, Range: 107.00\n",
            "  CV: 0.122\n",
            "glucose_postprandial:\n",
            "  Média: 160.07, Desvio: 30.91\n",
            "  Min: 70.00, Max: 287.00, Range: 217.00\n",
            "  CV: 0.193\n",
            "insulin_level:\n",
            "  Média: 9.06, Desvio: 4.95\n",
            "  Min: 2.00, Max: 30.76, Range: 28.76\n",
            "  CV: 0.546\n",
            "hba1c:\n",
            "  Média: 6.52, Desvio: 0.81\n",
            "  Min: 4.00, Max: 9.80, Range: 5.80\n",
            "  CV: 0.125\n",
            "diabetes_risk_score:\n",
            "  Média: 30.21, Desvio: 9.07\n",
            "  Min: 2.70, Max: 67.20, Range: 64.50\n",
            "  CV: 0.300\n",
            "\n",
            "ESTRATÉGIA DE SCALING:\n",
            "------------------------------\n",
            "Variáveis com alta variabilidade: ['alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'hypertension_history', 'cholesterol_total', 'ldl_cholesterol', 'triglycerides', 'glucose_fasting', 'glucose_postprandial', 'insulin_level']\n",
            "Variáveis com valores negativos: []\n",
            "\n",
            "Escolha do scaler: StandardScaler\n",
            "Motivo: StandardScaler é robusto e funciona bem com a maioria dos algoritmos\n",
            "\n",
            "Aplicando StandardScaler...\n",
            "✅ Scaling concluído!\n",
            "\n",
            "Verificação do scaling:\n",
            "------------------------------\n",
            "age: Média=0.000, Desvio=1.000\n",
            "alcohol_consumption_per_week: Média=0.000, Desvio=1.000\n",
            "physical_activity_minutes_per_week: Média=-0.000, Desvio=1.000\n",
            "diet_score: Média=-0.000, Desvio=1.000\n",
            "sleep_hours_per_day: Média=-0.000, Desvio=1.000\n",
            "\n",
            "Shape final: (80000, 45)\n",
            "Tipo de dados: float64    23\n",
            "bool       22\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Normalização/Padronização de variáveis numéricas\n",
        "print(\"=\"*60)\n",
        "print(\"NORMALIZAÇÃO/PADRONIZAÇÃO DE VARIÁVEIS NUMÉRICAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identificar variáveis numéricas após encoding\n",
        "numeric_columns_after_encoding = X_train_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Variáveis numéricas após encoding: {len(numeric_columns_after_encoding)}\")\n",
        "print(f\"Colunas: {numeric_columns_after_encoding}\")\n",
        "\n",
        "# Analisar distribuições das variáveis numéricas\n",
        "print(f\"\\nAnálise das distribuições:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "scaling_analysis = {}\n",
        "for col in numeric_columns_after_encoding:\n",
        "    mean_val = X_train_encoded[col].mean()\n",
        "    std_val = X_train_encoded[col].std()\n",
        "    min_val = X_train_encoded[col].min()\n",
        "    max_val = X_train_encoded[col].max()\n",
        "    range_val = max_val - min_val\n",
        "    \n",
        "    # Calcular coeficiente de variação\n",
        "    cv = std_val / mean_val if mean_val != 0 else 0\n",
        "    \n",
        "    scaling_analysis[col] = {\n",
        "        'mean': mean_val,\n",
        "        'std': std_val,\n",
        "        'min': min_val,\n",
        "        'max': max_val,\n",
        "        'range': range_val,\n",
        "        'cv': cv\n",
        "    }\n",
        "    \n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  Média: {mean_val:.2f}, Desvio: {std_val:.2f}\")\n",
        "    print(f\"  Min: {min_val:.2f}, Max: {max_val:.2f}, Range: {range_val:.2f}\")\n",
        "    print(f\"  CV: {cv:.3f}\")\n",
        "\n",
        "# Decidir estratégia de scaling\n",
        "print(f\"\\nESTRATÉGIA DE SCALING:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Variáveis com alta variabilidade (CV > 0.5) ou grande range\n",
        "high_variability_vars = [col for col, info in scaling_analysis.items() \n",
        "                        if info['cv'] > 0.5 or info['range'] > 100]\n",
        "\n",
        "# Variáveis com valores negativos (não podem usar MinMaxScaler)\n",
        "negative_vars = [col for col, info in scaling_analysis.items() \n",
        "                if info['min'] < 0]\n",
        "\n",
        "print(f\"Variáveis com alta variabilidade: {high_variability_vars}\")\n",
        "print(f\"Variáveis com valores negativos: {negative_vars}\")\n",
        "\n",
        "# Escolher StandardScaler como padrão (mais robusto)\n",
        "scaler_choice = 'StandardScaler'\n",
        "print(f\"\\nEscolha do scaler: {scaler_choice}\")\n",
        "print(\"Motivo: StandardScaler é robusto e funciona bem com a maioria dos algoritmos\")\n",
        "\n",
        "# Aplicar scaling\n",
        "print(f\"\\nAplicando {scaler_choice}...\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Aplicar apenas nas variáveis numéricas\n",
        "X_train_scaled = X_train_encoded.copy()\n",
        "X_train_scaled[numeric_columns_after_encoding] = scaler.fit_transform(\n",
        "    X_train_encoded[numeric_columns_after_encoding]\n",
        ")\n",
        "\n",
        "print(f\"✅ Scaling concluído!\")\n",
        "\n",
        "# Verificar resultado do scaling\n",
        "print(f\"\\nVerificação do scaling:\")\n",
        "print(\"-\" * 30)\n",
        "for col in numeric_columns_after_encoding[:5]:  # Mostrar apenas as 5 primeiras\n",
        "    mean_after = X_train_scaled[col].mean()\n",
        "    std_after = X_train_scaled[col].std()\n",
        "    print(f\"{col}: Média={mean_after:.3f}, Desvio={std_after:.3f}\")\n",
        "\n",
        "print(f\"\\nShape final: {X_train_scaled.shape}\")\n",
        "print(f\"Tipo de dados: {X_train_scaled.dtypes.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CRIAÇÃO DE PIPELINE DE TRANSFORMAÇÃO\n",
            "============================================================\n",
            "Criando pipeline de transformação...\n",
            "✅ Pipeline criado com sucesso!\n",
            "Etapas do pipeline:\n",
            "  1. outlier_treatment: OutlierTreatment\n",
            "  2. categorical_encoding: CategoricalEncoder\n",
            "  3. scaling: StandardScaler\n",
            "\n",
            "Testando pipeline no conjunto de treino...\n",
            "Shape após pipeline: (80000, 45)\n",
            "Tipo de dados: <class 'numpy.ndarray'>\n",
            "\n",
            "Comparação com processamento manual:\n",
            "Pipeline: (80000, 45)\n",
            "Manual:   (80000, 45)\n",
            "Diferença: 0 colunas\n"
          ]
        }
      ],
      "source": [
        "# Criação de Pipeline de Transformação\n",
        "print(\"=\"*60)\n",
        "print(\"CRIAÇÃO DE PIPELINE DE TRANSFORMAÇÃO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class OutlierTreatment:\n",
        "    \"\"\"Classe customizada para tratamento de outliers\"\"\"\n",
        "    \n",
        "    def __init__(self, outlier_vars, method='iqr', factor=1.5):\n",
        "        self.outlier_vars = outlier_vars\n",
        "        self.method = method\n",
        "        self.factor = factor\n",
        "        self.bounds_ = {}\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        for var in self.outlier_vars:\n",
        "            if var in X.columns:\n",
        "                if self.method == 'iqr':\n",
        "                    Q1 = X[var].quantile(0.25)\n",
        "                    Q3 = X[var].quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - self.factor * IQR\n",
        "                    upper_bound = Q3 + self.factor * IQR\n",
        "                elif self.method == 'zscore':\n",
        "                    mean = X[var].mean()\n",
        "                    std = X[var].std()\n",
        "                    lower_bound = mean - self.factor * std\n",
        "                    upper_bound = mean + self.factor * std\n",
        "                \n",
        "                self.bounds_[var] = (lower_bound, upper_bound)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        for var, (lower, upper) in self.bounds_.items():\n",
        "            if var in X_transformed.columns:\n",
        "                X_transformed[var] = np.where(X_transformed[var] < lower, lower, X_transformed[var])\n",
        "                X_transformed[var] = np.where(X_transformed[var] > upper, upper, X_transformed[var])\n",
        "        return X_transformed\n",
        "\n",
        "class CategoricalEncoder:\n",
        "    \"\"\"Classe customizada para encoding de variáveis categóricas\"\"\"\n",
        "    \n",
        "    def __init__(self, onehot_vars, label_vars):\n",
        "        self.onehot_vars = onehot_vars\n",
        "        self.label_vars = label_vars\n",
        "        self.label_encoders_ = {}\n",
        "        self.onehot_categories_ = {}\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        # Fit label encoders\n",
        "        for var in self.label_vars:\n",
        "            if var in X.columns:\n",
        "                le = LabelEncoder()\n",
        "                le.fit(X[var])\n",
        "                self.label_encoders_[var] = le\n",
        "        \n",
        "        # Store onehot categories\n",
        "        for var in self.onehot_vars:\n",
        "            if var in X.columns:\n",
        "                self.onehot_categories_[var] = X[var].unique()\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        \n",
        "        # Apply label encoding\n",
        "        for var, le in self.label_encoders_.items():\n",
        "            if var in X_transformed.columns:\n",
        "                X_transformed[var] = le.transform(X_transformed[var])\n",
        "        \n",
        "        # Apply onehot encoding\n",
        "        for var in self.onehot_vars:\n",
        "            if var in X_transformed.columns:\n",
        "                dummies = pd.get_dummies(X_transformed[var], prefix=var, drop_first=True)\n",
        "                X_transformed = X_transformed.drop(columns=[var])\n",
        "                X_transformed = pd.concat([X_transformed, dummies], axis=1)\n",
        "        \n",
        "        return X_transformed\n",
        "\n",
        "# Criar pipeline completo\n",
        "print(\"Criando pipeline de transformação...\")\n",
        "\n",
        "# Definir transformadores\n",
        "outlier_transformer = OutlierTreatment(vars_to_treat, method='iqr', factor=1.5)\n",
        "categorical_transformer = CategoricalEncoder(onehot_vars, label_vars)\n",
        "scaler_transformer = StandardScaler()\n",
        "\n",
        "# Criar pipeline\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('outlier_treatment', outlier_transformer),\n",
        "    ('categorical_encoding', categorical_transformer),\n",
        "    ('scaling', scaler_transformer)\n",
        "])\n",
        "\n",
        "print(\"✅ Pipeline criado com sucesso!\")\n",
        "print(\"Etapas do pipeline:\")\n",
        "for i, (name, transformer) in enumerate(preprocessing_pipeline.steps):\n",
        "    print(f\"  {i+1}. {name}: {type(transformer).__name__}\")\n",
        "\n",
        "# Testar pipeline no conjunto de treino\n",
        "print(f\"\\nTestando pipeline no conjunto de treino...\")\n",
        "X_train_pipeline = preprocessing_pipeline.fit_transform(X_train)\n",
        "\n",
        "print(f\"Shape após pipeline: {X_train_pipeline.shape}\")\n",
        "print(f\"Tipo de dados: {type(X_train_pipeline)}\")\n",
        "\n",
        "# Verificar se o resultado é similar ao processamento manual\n",
        "print(f\"\\nComparação com processamento manual:\")\n",
        "print(f\"Pipeline: {X_train_pipeline.shape}\")\n",
        "print(f\"Manual:   {X_train_scaled.shape}\")\n",
        "print(f\"Diferença: {abs(X_train_pipeline.shape[1] - X_train_scaled.shape[1])} colunas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "VALIDAÇÃO DAS TRANSFORMAÇÕES\n",
            "============================================================\n",
            "Aplicando pipeline no conjunto de teste...\n",
            "Shape do conjunto de teste transformado: (20000, 45)\n",
            "Tipo de dados: <class 'numpy.ndarray'>\n",
            "\n",
            "Verificação de valores nulos:\n",
            "Train: 0\n",
            "Test:  0\n",
            "\n",
            "Verificação de valores infinitos:\n",
            "Train: 0\n",
            "Test:  0\n",
            "\n",
            "Estatísticas básicas do conjunto de treino:\n",
            "Média: 0.000\n",
            "Desvio padrão: 0.978\n",
            "Min: -3.964\n",
            "Max: 27.851\n",
            "\n",
            "Estatísticas básicas do conjunto de teste:\n",
            "Média: -0.000\n",
            "Desvio padrão: 0.971\n",
            "Min: -3.751\n",
            "Max: 27.851\n",
            "\n",
            "Comparação de distribuições (primeiras 5 features):\n",
            "Feature 0:\n",
            "  Train - Média: 0.000, Desvio: 1.000\n",
            "  Test  - Média: 0.008, Desvio: 1.004\n",
            "  Diferença - Média: 0.008, Desvio: 0.004\n",
            "Feature 1:\n",
            "  Train - Média: 0.000, Desvio: 1.000\n",
            "  Test  - Média: 0.002, Desvio: 0.996\n",
            "  Diferença - Média: 0.002, Desvio: 0.004\n",
            "Feature 2:\n",
            "  Train - Média: -0.000, Desvio: 1.000\n",
            "  Test  - Média: -0.011, Desvio: 0.979\n",
            "  Diferença - Média: 0.011, Desvio: 0.021\n",
            "Feature 3:\n",
            "  Train - Média: -0.000, Desvio: 1.000\n",
            "  Test  - Média: 0.003, Desvio: 0.997\n",
            "  Diferença - Média: 0.003, Desvio: 0.003\n",
            "Feature 4:\n",
            "  Train - Média: -0.000, Desvio: 1.000\n",
            "  Test  - Média: -0.006, Desvio: 1.000\n",
            "  Diferença - Média: 0.006, Desvio: 0.000\n",
            "\n",
            "Verificação do target:\n",
            "Train target shape: (80000,)\n",
            "Test target shape: (20000,)\n",
            "Train target distribution: diagnosed_diabetes\n",
            "1    0.6\n",
            "0    0.4\n",
            "Name: proportion, dtype: float64\n",
            "Test target distribution: diagnosed_diabetes\n",
            "1    0.6\n",
            "0    0.4\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "✅ Validação concluída com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Validação das Transformações\n",
        "print(\"=\"*60)\n",
        "print(\"VALIDAÇÃO DAS TRANSFORMAÇÕES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Aplicar pipeline no conjunto de teste\n",
        "print(\"Aplicando pipeline no conjunto de teste...\")\n",
        "X_test_transformed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "print(f\"Shape do conjunto de teste transformado: {X_test_transformed.shape}\")\n",
        "print(f\"Tipo de dados: {type(X_test_transformed)}\")\n",
        "\n",
        "# Verificar se não há valores nulos\n",
        "print(f\"\\nVerificação de valores nulos:\")\n",
        "print(f\"Train: {np.isnan(X_train_pipeline).sum()}\")\n",
        "print(f\"Test:  {np.isnan(X_test_transformed).sum()}\")\n",
        "\n",
        "# Verificar se há valores infinitos\n",
        "print(f\"\\nVerificação de valores infinitos:\")\n",
        "print(f\"Train: {np.isinf(X_train_pipeline).sum()}\")\n",
        "print(f\"Test:  {np.isinf(X_test_transformed).sum()}\")\n",
        "\n",
        "# Verificar estatísticas básicas\n",
        "print(f\"\\nEstatísticas básicas do conjunto de treino:\")\n",
        "print(f\"Média: {X_train_pipeline.mean():.3f}\")\n",
        "print(f\"Desvio padrão: {X_train_pipeline.std():.3f}\")\n",
        "print(f\"Min: {X_train_pipeline.min():.3f}\")\n",
        "print(f\"Max: {X_train_pipeline.max():.3f}\")\n",
        "\n",
        "print(f\"\\nEstatísticas básicas do conjunto de teste:\")\n",
        "print(f\"Média: {X_test_transformed.mean():.3f}\")\n",
        "print(f\"Desvio padrão: {X_test_transformed.std():.3f}\")\n",
        "print(f\"Min: {X_test_transformed.min():.3f}\")\n",
        "print(f\"Max: {X_test_transformed.max():.3f}\")\n",
        "\n",
        "# Verificar se as distribuições são similares\n",
        "print(f\"\\nComparação de distribuições (primeiras 5 features):\")\n",
        "for i in range(min(5, X_train_pipeline.shape[1])):\n",
        "    train_mean = X_train_pipeline[:, i].mean()\n",
        "    test_mean = X_test_transformed[:, i].mean()\n",
        "    train_std = X_train_pipeline[:, i].std()\n",
        "    test_std = X_test_transformed[:, i].std()\n",
        "    \n",
        "    print(f\"Feature {i}:\")\n",
        "    print(f\"  Train - Média: {train_mean:.3f}, Desvio: {train_std:.3f}\")\n",
        "    print(f\"  Test  - Média: {test_mean:.3f}, Desvio: {test_std:.3f}\")\n",
        "    print(f\"  Diferença - Média: {abs(train_mean - test_mean):.3f}, Desvio: {abs(train_std - test_std):.3f}\")\n",
        "\n",
        "# Verificar se o target não foi afetado\n",
        "print(f\"\\nVerificação do target:\")\n",
        "print(f\"Train target shape: {y_train.shape}\")\n",
        "print(f\"Test target shape: {y_test.shape}\")\n",
        "print(f\"Train target distribution: {y_train.value_counts(normalize=True).round(3)}\")\n",
        "print(f\"Test target distribution: {y_test.value_counts(normalize=True).round(3)}\")\n",
        "\n",
        "print(f\"\\n✅ Validação concluída com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SALVANDO DADOS PROCESSADOS E PIPELINE\n",
            "============================================================\n",
            "Salvando dados processados...\n",
            "✅ Dados processados salvos:\n",
            "  X_train: ../data/processed/X_train_processed.csv ((80000, 45))\n",
            "  X_test:  ../data/processed/X_test_processed.csv ((20000, 45))\n",
            "  y_train: ../data/processed/y_train.csv ((80000,))\n",
            "  y_test:  ../data/processed/y_test.csv ((20000,))\n",
            "\n",
            "Salvando pipeline de transformação...\n",
            "✅ Pipeline salvo: ../models/preprocessing_pipeline.pkl\n",
            "✅ Informações de transformação salvas: ../models/transformation_info.pkl\n",
            "\n",
            "RESUMO DAS TRANSFORMAÇÕES:\n",
            "----------------------------------------\n",
            "Dataset original: (100000, 31)\n",
            "Dataset processado: (80000, 45)\n",
            "Variáveis tratadas para outliers: 2\n",
            "Variáveis com One-Hot Encoding: 7\n",
            "Variáveis com Label Encoding: 0\n",
            "Variáveis normalizadas: 23\n",
            "Features finais: 45\n",
            "\n",
            "🎯 PRÓXIMOS PASSOS:\n",
            "--------------------\n",
            "1. Os dados estão prontos para treinamento de modelos\n",
            "2. Use X_train_pipeline e y_train para treinar modelos\n",
            "3. Use X_test_transformed e y_test para avaliação\n",
            "4. O pipeline pode ser reutilizado com novos dados\n",
            "5. Considere feature selection e engenharia de features adicionais\n",
            "\n",
            "✅ Preparação de dados concluída com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Salvar dados processados e pipeline\n",
        "print(\"=\"*60)\n",
        "print(\"SALVANDO DADOS PROCESSADOS E PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Criar diretórios se não existirem\n",
        "os.makedirs('../data/processed', exist_ok=True)\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Salvar dados processados\n",
        "print(\"Salvando dados processados...\")\n",
        "\n",
        "# Converter arrays numpy para DataFrames para facilitar o uso\n",
        "feature_names = [f'feature_{i}' for i in range(X_train_pipeline.shape[1])]\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_pipeline, columns=feature_names, index=train_indices)\n",
        "X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names, index=test_indices)\n",
        "\n",
        "# Salvar como CSV\n",
        "X_train_df.to_csv('../data/processed/X_train_processed.csv')\n",
        "X_test_df.to_csv('../data/processed/X_test_processed.csv')\n",
        "y_train.to_csv('../data/processed/y_train.csv')\n",
        "y_test.to_csv('../data/processed/y_test.csv')\n",
        "\n",
        "print(\"✅ Dados processados salvos:\")\n",
        "print(f\"  X_train: ../data/processed/X_train_processed.csv ({X_train_df.shape})\")\n",
        "print(f\"  X_test:  ../data/processed/X_test_processed.csv ({X_test_df.shape})\")\n",
        "print(f\"  y_train: ../data/processed/y_train.csv ({y_train.shape})\")\n",
        "print(f\"  y_test:  ../data/processed/y_test.csv ({y_test.shape})\")\n",
        "\n",
        "# Salvar pipeline de transformação\n",
        "print(f\"\\nSalvando pipeline de transformação...\")\n",
        "joblib.dump(preprocessing_pipeline, '../models/preprocessing_pipeline.pkl')\n",
        "\n",
        "print(\"✅ Pipeline salvo: ../models/preprocessing_pipeline.pkl\")\n",
        "\n",
        "# Salvar informações sobre as transformações\n",
        "transformation_info = {\n",
        "    'outlier_vars': vars_to_treat,\n",
        "    'onehot_vars': onehot_vars,\n",
        "    'label_vars': label_vars,\n",
        "    'numeric_vars': numeric_columns_after_encoding,\n",
        "    'feature_names': feature_names,\n",
        "    'outlier_treatment_info': outlier_treatment_info,\n",
        "    'categorical_analysis': categorical_analysis,\n",
        "    'scaling_analysis': scaling_analysis\n",
        "}\n",
        "\n",
        "joblib.dump(transformation_info, '../models/transformation_info.pkl')\n",
        "print(\"✅ Informações de transformação salvas: ../models/transformation_info.pkl\")\n",
        "\n",
        "# Criar resumo das transformações\n",
        "print(f\"\\nRESUMO DAS TRANSFORMAÇÕES:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Dataset original: {df.shape}\")\n",
        "print(f\"Dataset processado: {X_train_pipeline.shape}\")\n",
        "print(f\"Variáveis tratadas para outliers: {len(vars_to_treat)}\")\n",
        "print(f\"Variáveis com One-Hot Encoding: {len(onehot_vars)}\")\n",
        "print(f\"Variáveis com Label Encoding: {len(label_vars)}\")\n",
        "print(f\"Variáveis normalizadas: {len(numeric_columns_after_encoding)}\")\n",
        "print(f\"Features finais: {X_train_pipeline.shape[1]}\")\n",
        "\n",
        "print(f\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
        "print(\"-\" * 20)\n",
        "print(\"1. Os dados estão prontos para treinamento de modelos\")\n",
        "print(\"2. Use X_train_pipeline e y_train para treinar modelos\")\n",
        "print(\"3. Use X_test_transformed e y_test para avaliação\")\n",
        "print(\"4. O pipeline pode ser reutilizado com novos dados\")\n",
        "print(\"5. Considere feature selection e engenharia de features adicionais\")\n",
        "\n",
        "print(f\"\\n✅ Preparação de dados concluída com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📊 Resumo Executivo da Preparação de Dados\n",
        "\n",
        "## 🎯 Objetivo\n",
        "Este notebook implementa o pipeline completo de preparação de dados para treinamento de modelos de Machine Learning, seguindo as recomendações da análise exploratória.\n",
        "\n",
        "## 🔧 Transformações Implementadas\n",
        "\n",
        "### 1. Divisão Estratificada dos Dados\n",
        "- **Split**: 80% treino / 20% teste\n",
        "- **Estratificação**: Mantém proporção das classes\n",
        "- **Random State**: 42 (reprodutibilidade)\n",
        "\n",
        "### 2. Tratamento de Outliers\n",
        "- **Método**: Capping (Winsorization) usando IQR\n",
        "- **Fator**: 1.5 × IQR\n",
        "- **Variáveis tratadas**: Identificadas automaticamente baseado em % de outliers\n",
        "\n",
        "### 3. Encoding de Variáveis Categóricas\n",
        "- **One-Hot Encoding**: Variáveis com ≤ 5 categorias\n",
        "- **Label Encoding**: Variáveis com > 5 categorias\n",
        "- **Estratégia**: Automática baseada no número de categorias únicas\n",
        "\n",
        "### 4. Normalização/Padronização\n",
        "- **Método**: StandardScaler\n",
        "- **Aplicação**: Apenas em variáveis numéricas\n",
        "- **Resultado**: Média ≈ 0, Desvio padrão ≈ 1\n",
        "\n",
        "## 📁 Arquivos Gerados\n",
        "\n",
        "### Dados Processados\n",
        "- `X_train_processed.csv`: Features de treino\n",
        "- `X_test_processed.csv`: Features de teste\n",
        "- `y_train.csv`: Target de treino\n",
        "- `y_test.csv`: Target de teste\n",
        "\n",
        "### Modelos e Pipeline\n",
        "- `preprocessing_pipeline.pkl`: Pipeline completo de transformação\n",
        "- `transformation_info.pkl`: Metadados das transformações\n",
        "\n",
        "## ✅ Validações Realizadas\n",
        "\n",
        "1. **Qualidade dos Dados**\n",
        "   - ✅ Sem valores nulos\n",
        "   - ✅ Sem valores infinitos\n",
        "   - ✅ Tipos de dados consistentes\n",
        "\n",
        "2. **Distribuições**\n",
        "   - ✅ Normalização aplicada corretamente\n",
        "   - ✅ Distribuições similares entre train/test\n",
        "   - ✅ Target não afetado pelas transformações\n",
        "\n",
        "3. **Reprodutibilidade**\n",
        "   - ✅ Pipeline salvo e reutilizável\n",
        "   - ✅ Random states fixados\n",
        "   - ✅ Metadados preservados\n",
        "\n",
        "## 🚀 Próximos Passos\n",
        "\n",
        "1. **Treinamento de Modelos**\n",
        "   - Usar dados processados para treinar diferentes algoritmos\n",
        "   - Implementar validação cruzada\n",
        "   - Otimizar hiperparâmetros\n",
        "\n",
        "2. **Feature Engineering**\n",
        "   - Criar features derivadas\n",
        "   - Aplicar feature selection\n",
        "   - Considerar interações entre variáveis\n",
        "\n",
        "3. **Avaliação**\n",
        "   - Métricas apropriadas para classificação\n",
        "   - Análise de importância das features\n",
        "   - Interpretabilidade do modelo\n",
        "\n",
        "---\n",
        "*Pipeline de preparação implementado com sucesso!*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
